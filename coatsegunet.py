# -*- coding: utf-8 -*-
"""CoAtSegUNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LwuL5ILx08bk5aBnGXmt9EsE6FZO5lZt
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------
# MBConv Block
# -------------------
class MBConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, dilation=1):
        super(MBConvBlock, self).__init__()
        self.expand = nn.Conv3d(in_channels, in_channels * 4, kernel_size=1)
        self.depthwise = nn.Conv3d(in_channels * 4, in_channels * 4, kernel_size=3, padding=dilation,
                                   dilation=dilation, groups=in_channels * 4)
        self.project = nn.Conv3d(in_channels * 4, out_channels, kernel_size=1)
        self.norm = nn.BatchNorm3d(out_channels)

    def forward(self, x):
        residual = x
        x = F.relu(self.expand(x))
        x = F.relu(self.depthwise(x))
        x = self.project(x)
        x = self.norm(x)
        return x + residual

# -------------------
# Relative Attention Block (simplified)
# -------------------
class AttentionBlock(nn.Module):
    def __init__(self, dim):
        super(AttentionBlock, self).__init__()
        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        B, C, D, H, W = x.shape
        x = x.view(B, C, -1).permute(0, 2, 1)  # B, N, C
        qkv = self.qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: F.normalize(t, dim=-1), qkv)
        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) / (C ** 0.5), dim=-1)
        x = torch.bmm(attn, v)
        x = self.proj(x)
        x = self.norm(x)
        x = x.permute(0, 2, 1).view(B, C, D, H, W)
        return x

# -------------------
# MLDR Decoder Block
# -------------------
class MLDRBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(MLDRBlock, self).__init__()
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1)
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=3, dilation=3)
        self.conv3 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=5, dilation=5)
        self.out_conv = nn.Conv3d(out_channels * 3, out_channels, kernel_size=1)

    def forward(self, x):
        x1 = F.relu(self.conv1(x))
        x2 = F.relu(self.conv2(x))
        x3 = F.relu(self.conv3(x))
        x_cat = torch.cat([x1, x2, x3], dim=1)
        return self.out_conv(x_cat)

# -------------------
# CoAt U SegNet
# -------------------
class CoAtUSegNet(nn.Module):
    def __init__(self, in_channels=1, base_channels=32):
        super(CoAtUSegNet, self).__init__()
        # Encoder
        self.mb1 = MBConvBlock(in_channels, base_channels, dilation=1)
        self.mb2 = MBConvBlock(base_channels, base_channels * 2, dilation=3)
        self.attn1 = AttentionBlock(base_channels * 2)
        self.attn2 = AttentionBlock(base_channels * 2)

        # Decoder
        self.up1 = nn.ConvTranspose3d(base_channels * 2, base_channels, kernel_size=2, stride=2)
        self.mldr1 = MLDRBlock(base_channels * 3, base_channels)
        self.up2 = nn.ConvTranspose3d(base_channels, base_channels, kernel_size=2, stride=2)
        self.mldr2 = MLDRBlock(base_channels * 2, base_channels)

        self.final = nn.Conv3d(base_channels, 1, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        f1 = self.mb1(x)
        f2 = self.mb2(f1)
        f3 = self.attn1(f2)
        f4 = self.attn2(f3)

        d1 = self.up1(f4)
        d1 = torch.cat([d1, f2], dim=1)
        d1 = self.mldr1(d1)

        d2 = self.up2(d1)
        d2 = torch.cat([d2, f1], dim=1)
        d2 = self.mldr2(d2)

        out = self.sigmoid(self.final(d2))
        return out

def dice_loss(pred, target, epsilon=1e-6):
    pred = pred.contiguous()
    target = target.contiguous()
    intersection = (pred * target).sum()
    return 1 - ((2. * intersection + epsilon) /
                (pred.sum() + target.sum() + epsilon))

def focal_loss(pred, target, alpha=0.8, gamma=2):
    pred = pred.clamp(1e-6, 1 - 1e-6)
    pt = pred * target + (1 - pred) * (1 - target)
    return -alpha * ((1 - pt) ** gamma) * torch.log(pt)

def combined_loss(pred, target):
    return 0.5 * dice_loss(pred, target) + 0.5 * focal_loss(pred, target)

train_dataset = StrokeCTDataset(
    image_dir='/data/images',
    mask_dir='/data/masks',
    transform=random_flip_3d  # optional
)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)

model = CoAtUSegNet().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
num_epochs = 100

for epoch in range(num_epochs):
    model.train()
    for images, masks in train_loader:  # You need to define DataLoader
        images, masks = images.cuda(), masks.cuda()
        outputs = model(images)
        loss = combined_loss(outputs, masks)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")